---
sidebar_position: 3
---

# Reflect

Generate reasoned responses using mental models and memories.

When you call **reflect**, Hindsight runs an agentic reasoning loop:
1. **Explores** mental models for structured understanding of key topics
2. **Recalls** relevant memories from the bank based on the query
3. **Reasons** through evidence applying the bank's disposition
4. **Learns** by creating new mental models when important patterns are discovered

The response includes the generated answer along with the facts and mental models that were used, providing full transparency into how the answer was derived.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeSnippet from '@site/src/components/CodeSnippet';

{/* Import raw source files */}
import reflectPy from '!!raw-loader!@site/examples/api/reflect.py';
import reflectMjs from '!!raw-loader!@site/examples/api/reflect.mjs';
import reflectSh from '!!raw-loader!@site/examples/api/reflect.sh';

:::info How Reflect Works
Learn about mental models and disposition-driven reasoning in the [Reflect Architecture](/developer/reflect) guide.
:::

:::tip Prerequisites
Make sure you've completed the [Quick Start](./quickstart) to install the client and start the server.
:::

## Basic usage

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-basic" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-basic" language="javascript" />
</TabItem>
<TabItem value="cli" label="CLI">
<CodeSnippet code={reflectSh} section="reflect-basic" language="bash" />
</TabItem>
</Tabs>

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `query` | string | required | Question or prompt |
| `budget` | string | "low" | Budget level: `low`, `mid`, `high` |
| `max_tokens` | int | 4096 | Maximum tokens for the response |
| `response_schema` | object | None | JSON Schema for [structured output](#structured-output) |
| `tags` | list | None | Filter memories and mental models by tags |
| `tags_match` | string | "any" | How to match tags: `any`, `all`, `any_strict`, `all_strict` |

### Response fields

| Field | Type | Description |
|-------|------|-------------|
| `text` | string | The generated answer text |
| `based_on` | object | Facts and mental models used to generate the response |
| `structured_output` | object | Parsed structured output (when `response_schema` provided) |
| `usage` | TokenUsage | Token usage metrics for the LLM call |

The `usage` field contains:
- `input_tokens`: Number of input/prompt tokens consumed
- `output_tokens`: Number of output/completion tokens generated
- `total_tokens`: Sum of input and output tokens

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-with-params" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-with-params" language="javascript" />
</TabItem>
</Tabs>

## Mental models

When a bank has a mission set, the reflect agent can draw on mental models—structured knowledge about key topics. The agent is agentic: it decides which mental models to consult based on the query.

During reflect, the agent has access to these tools:

| Tool | Purpose |
|------|---------|
| `list_mental_models()` | See available mental models |
| `get_mental_model(id)` | Read observations and evidence |
| `recall(query)` | Search raw memories |
| `expand(memory_ids)` | Load full document context |
| `learn(name, description)` | Create a new mental model to track a pattern |

### Learned mental models

If the agent discovers an important pattern during reasoning, it can create a "learned" mental model to track it going forward. For example, if asked about customer pricing feedback and the agent finds scattered information, it might create a "Pricing Feedback" mental model for future systematic tracking.

See [Mental Models API](./mental-models) for managing mental models.

## Disposition influence

The bank's disposition affects how reflect interprets information:

| Trait | Low (1) | High (5) |
|-------|---------|----------|
| **Skepticism** | Trusting, accepts claims | Questions and doubts claims |
| **Literalism** | Flexible interpretation | Exact, literal interpretation |
| **Empathy** | Detached, fact-focused | Considers emotional context |

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-disposition" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-disposition" language="javascript" />
</TabItem>
</Tabs>

## Using sources

The `based_on` field shows which memories and mental models informed the response:

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-sources" language="python" />
</TabItem>
<TabItem value="node" label="Node.js">
<CodeSnippet code={reflectMjs} section="reflect-sources" language="javascript" />
</TabItem>
</Tabs>

This enables:
- **Transparency** — users see why the bank said something
- **Verification** — check if the response is grounded in facts
- **Debugging** — understand retrieval quality

## Structured output

For applications that need to process responses programmatically, you can request structured output by providing a JSON Schema via `response_schema`. When provided, the response includes a `structured_output` field with the LLM response parsed according to the schema. The `text` field will be empty since only a single LLM call is made for efficiency.

The easiest way to define a schema is using **Pydantic models**:

<Tabs>
<TabItem value="python" label="Python">

```python
from pydantic import BaseModel
from hindsight_client import Hindsight

# Define your response structure with Pydantic
class HiringRecommendation(BaseModel):
    recommendation: str
    confidence: str  # "low", "medium", "high"
    key_factors: list[str]
    risks: list[str] = []

with Hindsight() as client:
    response = client.reflect(
        bank_id="hiring-team",
        query="Should we hire Alice for the ML team lead position?",
        response_schema=HiringRecommendation.model_json_schema(),
    )

    # Parse structured output into Pydantic model
    result = HiringRecommendation.model_validate(response.structured_output)
    print(f"Recommendation: {result.recommendation}")
    print(f"Confidence: {result.confidence}")
    print(f"Key factors: {result.key_factors}")
```

</TabItem>
<TabItem value="node" label="Node.js">

```javascript
import { Hindsight } from "@anthropic-ai/hindsight";

const client = new Hindsight();

// Define JSON schema directly
const responseSchema = {
  type: "object",
  properties: {
    recommendation: { type: "string" },
    confidence: { type: "string", enum: ["low", "medium", "high"] },
    key_factors: { type: "array", items: { type: "string" } },
    risks: { type: "array", items: { type: "string" } },
  },
  required: ["recommendation", "confidence", "key_factors"],
};

const response = await client.reflect({
  bankId: "hiring-team",
  query: "Should we hire Alice for the ML team lead position?",
  responseSchema: responseSchema,
});

// Structured output
console.log(response.structuredOutput.recommendation);
console.log(response.structuredOutput.keyFactors);
```

</TabItem>
<TabItem value="cli" label="CLI">

First, create a JSON schema file `schema.json`:
```json
{
  "type": "object",
  "properties": {
    "recommendation": {"type": "string"},
    "confidence": {"type": "string", "enum": ["low", "medium", "high"]},
    "key_factors": {"type": "array", "items": {"type": "string"}}
  },
  "required": ["recommendation", "confidence", "key_factors"]
}
```

Then use the `--schema` flag:
```bash
hindsight memory reflect hiring-team \
  "Should we hire Alice for the ML team lead position?" \
  --schema schema.json
```

</TabItem>
</Tabs>

| Use Case | Why Structured Output Helps |
|----------|----------------------------|
| **Decision pipelines** | Parse recommendations into workflow systems |
| **Dashboards** | Extract confidence scores, risk factors for visualization |
| **Multi-agent systems** | Pass structured data between agents |
| **Auditing** | Log structured decisions with clear reasoning |

**Tips:**
- Use Pydantic's `model_json_schema()` for type-safe schema generation
- Use `model_validate()` to parse the response back into your Pydantic model
- Keep schemas focused — extract only what you need
- Use `Optional` fields for data that may not always be available

## Filter by tags

Reflect supports tag filtering to scope which memories and mental models are considered during reasoning. This is essential for multi-user scenarios.

<Tabs>
<TabItem value="python" label="Python">
<CodeSnippet code={reflectPy} section="reflect-with-tags" language="python" />
</TabItem>
</Tabs>

The `tags_match` parameter controls how tags are matched:

| Mode | Behavior |
|------|----------|
| `any` | OR matching, includes untagged items |
| `all` | AND matching, includes untagged items |
| `any_strict` | OR matching, excludes untagged items |
| `all_strict` | AND matching, excludes untagged items |

See [Retain API](./retain#tagging-memories) for how to tag memories and [Mental Models API](./mental-models#tags-and-scoping) for tagging mental models.
